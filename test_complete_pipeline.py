#!/usr/bin/env python3
"""
Prueba completa del pipeline de app.py
Verifica que la extracciÃ³n de caracterÃ­sticas y predicciÃ³n funcionen correctamente
"""

import numpy as np
import pandas as pd
import pickle
import cv2
from skimage import feature, color
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from PIL import Image
import requests
from io import BytesIO
import warnings
import umap.umap_ as umap
from concurrent.futures import ThreadPoolExecutor

warnings.filterwarnings('ignore')

# Definir las clases necesarias (copiadas de app.py)
class KMeansCustom:
    """ImplementaciÃ³n personalizada del algoritmo K-means"""
    def __init__(self, n_clusters: int = 8, max_iters: int = 300, tol: float = 1e-4, random_state: int = 42):
        self.n_clusters = n_clusters
        self.max_iters = max_iters
        self.tol = tol
        self.random_state = random_state
        self.centroids_ = None
        self.labels_ = None
        self.inertia_ = None
        self.n_iter_ = None

    def predict(self, X):
        """Predice clusters para nuevos datos"""
        if self.centroids_ is None:
            raise ValueError("Modelo no entrenado")
        distances = np.sqrt(((X - self.centroids_[:, np.newaxis])**2).sum(axis=2))
        return np.argmin(distances, axis=0)

class VisualClusteringAnalyzer:
    """Analizador de clustering para caracterÃ­sticas visuales"""
    def __init__(self):
        self.features = None
        self.movie_metadata = None
        self.kmeans_model = None
        self.scaler = StandardScaler()
        self.pca = PCA(n_components=2)
        self.features_scaled = None
        self.features_pca = None
        self.feature_columns = None
        self.kmeans_labels = None
        self.kmeans_silhouette = None
        self.kmeans_inertia = None

# Funciones de extracciÃ³n (copiadas de app.py)
def extract_hsv(img, bins=32):
    """Extrae histogramas HSV de una imagen - IGUAL QUE PIPELINE.PY"""
    hsv = color.rgb2hsv(img)
    h = np.histogram(hsv[:, :, 0], bins=bins, range=(0, 1))[0]  # 32 bins
    s = np.histogram(hsv[:, :, 1], bins=bins, range=(0, 1))[0]  # 32 bins
    v = np.histogram(hsv[:, :, 2], bins=bins, range=(0, 1))[0]  # 32 bins
    return np.concatenate([h, s, v])  # Total: 96 features

def extract_lbp(img, P=8, R=1):
    """Extrae caracterÃ­sticas de Local Binary Pattern - IGUAL QUE PIPELINE.PY"""
    g = color.rgb2gray(img)
    g_uint8 = (g * 255).astype(np.uint8)
    lbp = feature.local_binary_pattern(g_uint8, P, R, method='uniform')
    n_bins = int(lbp.max() + 1)
    hist, _ = np.histogram(lbp, bins=n_bins, range=(0, n_bins), density=True)
    return hist  # Normalmente ~26 features

def extract_hu(img):
    """Extrae Hu Moments - IGUAL QUE PIPELINE.PY"""
    gray = color.rgb2gray(img)
    gray_uint8 = (gray * 255).astype(np.uint8)
    moments = cv2.moments(gray_uint8)
    hu = cv2.HuMoments(moments).flatten()
    return hu  # 7 features

def extract_all_features_original_pipeline(img):
    """
    Extrae caracterÃ­sticas usando el MISMO PIPELINE que pipeline.py
    HSV (96) + LBP (~26) + Hu (7) = ~129 features
    """
    try:
        with ThreadPoolExecutor(max_workers=3) as executor:
            future_hsv = executor.submit(extract_hsv, img)
            future_lbp = executor.submit(extract_lbp, img)
            future_hu = executor.submit(extract_hu, img)

            hsv_features = future_hsv.result()      # 96 features
            lbp_features = future_lbp.result()      # ~26 features
            hu_features = future_hu.result()        # 7 features

            # Total: ~129 features (igual que pipeline.py)
            all_features = np.concatenate([hsv_features, lbp_features, hu_features])
            return all_features

    except Exception as e:
        print(f"Error al extraer caracterÃ­sticas: {str(e)}")
        return None

def apply_simple_normalization(features):
    """
    Aplica normalizaciÃ³n simple basada en las estadÃ­sticas del dataset de entrenamiento
    """
    try:
        # Cargar dataset de entrenamiento para obtener estadÃ­sticas
        train_data = pd.read_csv('prim_reduced_features.csv')
        train_features = train_data.iloc[:, 2:].values  # Features 0-9

        # Calcular estadÃ­sticas del conjunto de entrenamiento
        train_mean = np.mean(train_features, axis=0)
        train_std = np.std(train_features, axis=0)

        # Si tenemos mÃ¡s de 10 features, necesitamos reducir dimensionalidad
        if len(features) > 10:
            print(f"ğŸ”„ Reduciendo {len(features)} caracterÃ­sticas a 10...")

            # Aplicar PCA simple para reducir a 10 componentes
            pca = PCA(n_components=10, random_state=42)

            # Crear datos sintÃ©ticos para ajustar PCA (simulando el proceso original)
            synthetic_data = np.random.normal(0, 1, (100, len(features)))
            pca.fit(synthetic_data)

            # Transformar la muestra actual
            features_reduced = pca.transform(features.reshape(1, -1))[0]
        else:
            features_reduced = features[:10] if len(features) >= 10 else np.pad(features, (0, 10-len(features)), 'constant')

        # Normalizar usando estadÃ­sticas del conjunto de entrenamiento
        normalized_features = (features_reduced - train_mean) / (train_std + 1e-8)

        return normalized_features

    except Exception as e:
        print(f"Error en normalizaciÃ³n simple: {str(e)}")
        # Fallback final: devolver caracterÃ­sticas normalizadas simples
        normalized = (features - np.mean(features)) / (np.std(features) + 1e-8)
        if len(normalized) >= 10:
            return normalized[:10]
        else:
            return np.pad(normalized, (0, 10-len(normalized)), 'constant')

def test_complete_pipeline():
    """Prueba el pipeline completo de extracciÃ³n y predicciÃ³n"""
    print("=" * 60)
    print("ğŸ§ª PRUEBA COMPLETA DEL PIPELINE")
    print("=" * 60)

    # 1. Cargar modelo
    print("ğŸ”„ Cargando modelo...")
    try:
        with open("visual_clustering_model/visual_clustering_analyzer.pkl", 'rb') as f:
            analyzer = pickle.load(f)
        print("âœ… Modelo cargado exitosamente")
        print(f"   - Features shape: {analyzer.features.shape}")
        print(f"   - K-means clusters: {analyzer.kmeans_model.n_clusters}")
    except Exception as e:
        print(f"âŒ Error cargando modelo: {str(e)}")
        return False

    # 2. Crear imagen de prueba
    print("\nğŸ”„ Creando imagen de prueba...")
    test_image = np.random.randint(0, 255, (150, 100, 3), dtype=np.uint8)
    print(f"âœ… Imagen creada: {test_image.shape}")

    # 3. Extraer caracterÃ­sticas
    print("\nğŸ”„ Extrayendo caracterÃ­sticas usando pipeline original...")
    features = extract_all_features_original_pipeline(test_image)
    if features is None:
        print("âŒ Error en extracciÃ³n de caracterÃ­sticas")
        return False
    print(f"âœ… CaracterÃ­sticas extraÃ­das: {len(features)}")
    print(f"   - Rango: [{np.min(features):.3f}, {np.max(features):.3f}]")

    # 4. Preprocesar caracterÃ­sticas
    print("\nğŸ”„ Preprocesando caracterÃ­sticas...")
    processed_features = apply_simple_normalization(features)
    if processed_features is None or len(processed_features) != 10:
        print("âŒ Error en preprocesamiento")
        return False
    print(f"âœ… CaracterÃ­sticas preprocesadas: {len(processed_features)}")
    print(f"   - Rango: [{np.min(processed_features):.3f}, {np.max(processed_features):.3f}]")

    # 5. Normalizar usando el scaler del modelo
    print("\nğŸ”„ Aplicando normalizaciÃ³n del modelo...")
    try:
        normalized_features = analyzer.scaler.transform(processed_features.reshape(1, -1))[0]
        print(f"âœ… NormalizaciÃ³n exitosa: {len(normalized_features)}")
        print(f"   - Rango: [{np.min(normalized_features):.3f}, {np.max(normalized_features):.3f}]")
    except Exception as e:
        print(f"âŒ Error en normalizaciÃ³n: {str(e)}")
        return False

    # 6. Hacer predicciÃ³n
    print("\nğŸ”„ Haciendo predicciÃ³n...")
    try:
        prediction = analyzer.kmeans_model.predict(normalized_features.reshape(1, -1))
        print(f"âœ… PredicciÃ³n exitosa: cluster {prediction[0]}")
    except Exception as e:
        print(f"âŒ Error en predicciÃ³n: {str(e)}")
        return False

    # 7. Encontrar pelÃ­culas similares
    print("\nğŸ”„ Buscando pelÃ­culas similares...")
    try:
        # Calcular distancias con todas las pelÃ­culas
        distances = []
        for idx in range(len(analyzer.features_scaled)):
            dist = np.linalg.norm(analyzer.features_scaled[idx] - normalized_features)
            distances.append((idx, dist))

        # Ordenar por distancia y tomar las 5 mÃ¡s cercanas
        distances.sort(key=lambda x: x[1])
        similar_indices = [idx for idx, _ in distances[:5]]

        print(f"âœ… Encontradas {len(similar_indices)} pelÃ­culas similares:")
        for i, idx in enumerate(similar_indices):
            print(f"   {i+1}. PelÃ­cula {idx} (distancia: {distances[i][1]:.3f})")

    except Exception as e:
        print(f"âŒ Error buscando similares: {str(e)}")
        return False

    print("\nğŸ‰ Â¡PIPELINE COMPLETO EXITOSO!")
    return True

def test_with_real_image():
    """Prueba con una imagen real descargada"""
    print("\n" + "=" * 60)
    print("ğŸ§ª PRUEBA CON IMAGEN REAL")
    print("=" * 60)

    # URL de imagen de prueba (poster de pelÃ­cula)
    test_url = "https://image.tmdb.org/t/p/w185/nZNUTxGsSB4nLEC9Bpa2xfu81qV.jpg"

    print(f"ğŸ”„ Descargando imagen desde: {test_url}")
    try:
        response = requests.get(test_url, timeout=10)
        img = Image.open(BytesIO(response.content)).convert("RGB")
        img_array = np.array(img)
        print(f"âœ… Imagen descargada: {img_array.shape}")
    except Exception as e:
        print(f"âŒ Error descargando imagen: {str(e)}")
        return False

    # Seguir el mismo pipeline
    print("\nğŸ”„ Procesando imagen real...")
    features = extract_all_features_original_pipeline(img_array)
    if features is None:
        print("âŒ Error en extracciÃ³n")
        return False

    processed_features = apply_simple_normalization(features)
    if processed_features is None:
        print("âŒ Error en preprocesamiento")
        return False

    print(f"âœ… Pipeline exitoso con imagen real")
    print(f"   - CaracterÃ­sticas extraÃ­das: {len(features)}")
    print(f"   - CaracterÃ­sticas finales: {len(processed_features)}")

    return True

def main():
    """FunciÃ³n principal"""
    print("ğŸš€ INICIANDO PRUEBAS DEL PIPELINE COMPLETO")

    # Prueba 1: Pipeline completo con imagen sintÃ©tica
    success1 = test_complete_pipeline()

    # Prueba 2: Pipeline con imagen real
    success2 = test_with_real_image()

    # Resumen
    print("\n" + "=" * 60)
    print("ğŸ“‹ RESUMEN FINAL")
    print("=" * 60)
    print(f"âœ… Pipeline completo: {'âœ… OK' if success1 else 'âŒ FALLO'}")
    print(f"âœ… Imagen real: {'âœ… OK' if success2 else 'âŒ FALLO'}")

    if success1 and success2:
        print("\nğŸ‰ Â¡TODAS LAS PRUEBAS EXITOSAS! El app.py deberÃ­a funcionar correctamente.")
    else:
        print("\nâš ï¸ Algunas pruebas fallaron. Revisa los errores arriba.")

if __name__ == "__main__":
    main()
