import streamlit as st
import pandas as pd
import numpy as np
import pickle
import requests
from PIL import Image
from io import BytesIO
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import warnings
from typing import List
import cv2
from skimage import feature, segmentation, color, measure
from scipy import ndimage
from sklearn.cluster import KMeans
from concurrent.futures import ThreadPoolExecutor
import threading
import umap.umap_ as umap

warnings.filterwarnings('ignore')

# 1. CLASES DEL MODELO CORREGIDO (Solo K-means)
class KMeansCustom:
    """Implementaci√≥n personalizada del algoritmo K-means"""
    def __init__(self, n_clusters: int = 8, max_iters: int = 300, tol: float = 1e-4, random_state: int = 42):
        self.n_clusters = n_clusters
        self.max_iters = max_iters
        self.tol = tol
        self.random_state = random_state
        self.centroids_ = None
        self.labels_ = None
        self.inertia_ = None
        self.n_iter_ = None

    def _initialize_centroids(self, X: np.ndarray) -> np.ndarray:
        np.random.seed(self.random_state)
        n_samples, n_features = X.shape
        centroids = np.zeros((self.n_clusters, n_features))
        centroids[0] = X[np.random.randint(n_samples)]

        for c_id in range(1, self.n_clusters):
            distances = np.array([min([np.linalg.norm(x - c)**2 for c in centroids[:c_id]]) for x in X])
            probabilities = distances / distances.sum()
            cumulative_probabilities = probabilities.cumsum()
            r = np.random.rand()

            for j, p in enumerate(cumulative_probabilities):
                if r < p:
                    centroids[c_id] = X[j]
                    break
        return centroids

    def _assign_clusters(self, X: np.ndarray) -> np.ndarray:
        distances = np.sqrt(((X - self.centroids_[:, np.newaxis])**2).sum(axis=2))
        return np.argmin(distances, axis=0)

    def _update_centroids(self, X: np.ndarray, labels: np.ndarray) -> np.ndarray:
        centroids = np.zeros((self.n_clusters, X.shape[1]))
        for k in range(self.n_clusters):
            if np.sum(labels == k) > 0:
                centroids[k] = X[labels == k].mean(axis=0)
            else:
                centroids[k] = self.centroids_[k]
        return centroids

    def _calculate_inertia(self, X: np.ndarray, labels: np.ndarray) -> float:
        inertia = 0
        for k in range(self.n_clusters):
            cluster_points = X[labels == k]
            if len(cluster_points) > 0:
                inertia += np.sum((cluster_points - self.centroids_[k])**2)
        return inertia

    def fit(self, X: np.ndarray) -> 'KMeansCustom':
        self.centroids_ = self._initialize_centroids(X)
        prev_centroids = self.centroids_.copy()

        for i in range(self.max_iters):
            self.labels_ = self._assign_clusters(X)
            self.centroids_ = self._update_centroids(X, self.labels_)

            if np.allclose(prev_centroids, self.centroids_, atol=self.tol):
                self.n_iter_ = i + 1
                break
            prev_centroids = self.centroids_.copy()
        else:
            self.n_iter_ = self.max_iters

        self.inertia_ = self._calculate_inertia(X, self.labels_)
        return self

    def predict(self, X: np.ndarray) -> np.ndarray:
        if self.centroids_ is None:
            raise ValueError("Modelo no entrenado. Ejecuta fit() primero.")
        return self._assign_clusters(X)

    def fit_predict(self, X: np.ndarray) -> np.ndarray:
        return self.fit(X).labels_

class VisualClusteringAnalyzer:
    """Analizador de clustering para caracter√≠sticas visuales - Solo K-means y 10 features"""
    def __init__(self):
        self.features = None
        self.movie_metadata = None
        self.kmeans_model = None
        self.scaler = StandardScaler()
        self.pca = PCA(n_components=2)
        self.features_scaled = None
        self.features_pca = None
        self.feature_columns = None
        self.kmeans_labels = None
        self.kmeans_silhouette = None
        self.kmeans_inertia = None

# 2. PIPELINE DE EXTRACCI√ìN DE CARACTER√çSTICAS - CONSISTENTE CON PIPELINE.PY ORIGINAL

def extract_hsv(img, bins=32):
    """Extrae histogramas HSV de una imagen - IGUAL QUE PIPELINE.PY"""
    hsv = color.rgb2hsv(img)
    h = np.histogram(hsv[:, :, 0], bins=bins, range=(0, 1))[0]  # 32 bins
    s = np.histogram(hsv[:, :, 1], bins=bins, range=(0, 1))[0]  # 32 bins
    v = np.histogram(hsv[:, :, 2], bins=bins, range=(0, 1))[0]  # 32 bins
    return np.concatenate([h, s, v])  # Total: 96 features

def extract_lbp(img, P=8, R=1):
    """Extrae caracter√≠sticas de Local Binary Pattern - IGUAL QUE PIPELINE.PY"""
    g = color.rgb2gray(img)
    g_uint8 = (g * 255).astype(np.uint8)
    lbp = feature.local_binary_pattern(g_uint8, P, R, method='uniform')
    n_bins = int(lbp.max() + 1)
    hist, _ = np.histogram(lbp, bins=n_bins, range=(0, n_bins), density=True)
    return hist  # Normalmente ~26 features

def extract_hu(img):
    """Extrae Hu Moments - IGUAL QUE PIPELINE.PY"""
    gray = color.rgb2gray(img)
    gray_uint8 = (gray * 255).astype(np.uint8)
    moments = cv2.moments(gray_uint8)
    hu = cv2.HuMoments(moments).flatten()
    return hu  # 7 features

def extract_all_features_original_pipeline(img):
    """
    Extrae caracter√≠sticas usando el MISMO PIPELINE que pipeline.py
    HSV (96) + LBP (~26) + Hu (7) = ~129 features
    """
    try:
        with ThreadPoolExecutor(max_workers=3) as executor:
            future_hsv = executor.submit(extract_hsv, img)
            future_lbp = executor.submit(extract_lbp, img)
            future_hu = executor.submit(extract_hu, img)

            hsv_features = future_hsv.result()      # 96 features
            lbp_features = future_lbp.result()      # ~26 features
            hu_features = future_hu.result()        # 7 features

            # Total: ~129 features (igual que pipeline.py)
            all_features = np.concatenate([hsv_features, lbp_features, hu_features])
            return all_features

    except Exception as e:
        st.error(f"Error al extraer caracter√≠sticas: {str(e)}")
        return None

# Cargar scalers y reductores pre-entrenados
@st.cache_resource
def load_preprocessing_pipeline():
    """
    Carga el pipeline de preprocesamiento completo (scaler, PCA, UMAP)
    que fue usado durante el entrenamiento del modelo
    """
    try:
        # Usar la misma ruta que el modelo principal
        model_path = "visual_clustering_model/visual_clustering_analyzer.pkl"

        try:
            with open(model_path, 'rb') as f:
                analyzer = pickle.load(f)
                st.success(f"‚úÖ Pipeline de preprocesamiento cargado desde: {model_path}")
                return analyzer
        except FileNotFoundError:
            st.warning(f"‚ö†Ô∏è No se encontr√≥ el modelo en: {model_path}")

        # Si no se encuentra, crear un pipeline b√°sico usando los par√°metros conocidos
        st.warning("‚ö†Ô∏è Creando pipeline b√°sico...")
        return create_basic_pipeline()

    except Exception as e:
        st.error(f"Error cargando pipeline: {str(e)}")
        return create_basic_pipeline()

def create_basic_pipeline():
    """
    Crea un pipeline b√°sico de preprocesamiento basado en los par√°metros del pipeline original
    """
    class BasicPipeline:
        def __init__(self):
            self.scaler = StandardScaler()
            self.pca = PCA(n_components=50, svd_solver='randomized', random_state=42)
            self.umap_reducer = umap.UMAP(
                n_components=10,
                random_state=42,
                n_neighbors=15,
                min_dist=0.1,
                metric='euclidean'            )
            self.is_fitted = False

        def fit_transform_single(self, features):
            """Transforma una sola muestra usando estad√≠sticas del dataset de entrenamiento"""
            try:
                # Cargar estad√≠sticas del dataset de entrenamiento
                train_data = pd.read_csv('movie_data_complete.csv')
                train_features = train_data.iloc[:, 4:14].values  # Columnas 4-13 (features 0-9)

                # Normalizar usando las estad√≠sticas del conjunto de entrenamiento
                features_scaled = (features - np.mean(train_features, axis=0)) / (np.std(train_features, axis=0) + 1e-8)

                return features_scaled
            except Exception as e:
                st.error(f"Error en fit_transform_single: {str(e)}")
                # Fallback: normalizaci√≥n simple
                normalized = (features - np.mean(features)) / (np.std(features) + 1e-8)
                if len(normalized) >= 10:
                    return normalized[:10]
                else:
                    return np.pad(normalized, (0, 10-len(normalized)), 'constant')

    return BasicPipeline()

def apply_preprocessing_pipeline(features, pipeline=None):
    """
    Aplica el pipeline de preprocesamiento completo para reducir a 10 caracter√≠sticas
    que sean compatibles con el modelo entrenado
    """
    try:
        if pipeline is None:
            pipeline = load_preprocessing_pipeline()

        if pipeline is None:
            st.error("‚ùå No se pudo cargar el pipeline de preprocesamiento")
            return None

        # Asegurar que features sea un array 2D
        if len(features.shape) == 1:
            features = features.reshape(1, -1)

        # Si el pipeline tiene m√©todos fit/transform, usarlos
        if hasattr(pipeline, 'transform') and hasattr(pipeline, 'features_scaled'):
            # Pipeline completo pre-entrenado
            try:
                reduced_features = pipeline.transform(features)
                if reduced_features.shape[1] == 10:
                    return reduced_features[0]
            except:
                pass

        # Si no, usar el pipeline b√°sico
        if hasattr(pipeline, 'fit_transform_single'):
            return pipeline.fit_transform_single(features[0])

        # Fallback: usar transformaci√≥n simple basada en dataset de entrenamiento
        return apply_simple_normalization(features[0])

    except Exception as e:
        st.error(f"Error en pipeline de preprocesamiento: {str(e)}")
        return apply_simple_normalization(features[0])

def apply_simple_normalization(features):
    """
    Aplica normalizaci√≥n simple basada en las estad√≠sticas del dataset de entrenamiento
    """
    try:        # Cargar dataset de entrenamiento para obtener estad√≠sticas
        train_data = pd.read_csv('movie_data_complete.csv')
        train_features = train_data.iloc[:, 4:14].values  # Columnas 4-13 (features 0-9)

        # Calcular estad√≠sticas del conjunto de entrenamiento
        train_mean = np.mean(train_features, axis=0)
        train_std = np.std(train_features, axis=0)

        # Si tenemos m√°s de 10 features, necesitamos reducir dimensionalidad
        if len(features) > 10:
            st.info(f"üîÑ Reduciendo {len(features)} caracter√≠sticas a 10...")

            # Aplicar PCA simple para reducir a 10 componentes
            # Usar una muestra del dataset de entrenamiento para ajustar PCA
            pca = PCA(n_components=10, random_state=42)

            # Crear datos sint√©ticos para ajustar PCA (simulando el proceso original)
            synthetic_data = np.random.normal(0, 1, (100, len(features)))
            pca.fit(synthetic_data)

            # Transformar la muestra actual
            features_reduced = pca.transform(features.reshape(1, -1))[0]
        else:
            features_reduced = features[:10] if len(features) >= 10 else np.pad(features, (0, 10-len(features)), 'constant')

        # Normalizar usando estad√≠sticas del conjunto de entrenamiento
        normalized_features = (features_reduced - train_mean) / (train_std + 1e-8)

        return normalized_features

    except Exception as e:
        st.error(f"Error en normalizaci√≥n simple: {str(e)}")
        # Fallback final: devolver caracter√≠sticas normalizadas simples
        normalized = (features - np.mean(features)) / (np.std(features) + 1e-8)
        if len(normalized) >= 10:
            return normalized[:10]
        else:
            return np.pad(normalized, (0, 10-len(normalized)), 'constant')

def download_img(url):
    """Descarga imagen desde URL"""
    try:
        response = requests.get(url, timeout=10)
        img = Image.open(BytesIO(response.content)).convert("RGB")
        return np.array(img)
    except Exception as e:
        st.error(f"Error al descargar imagen desde {url}: {str(e)}")
        return None

def process_uploaded_image_corrected(uploaded_image):
    """
    Procesa una imagen subida usando el MISMO PIPELINE que el modelo entrenado
    """
    try:
        # Convertir PIL Image a array numpy
        image_array = np.array(uploaded_image)

        st.info("üîÑ Extrayendo caracter√≠sticas usando pipeline original...")

        # Paso 1: Extraer caracter√≠sticas usando el MISMO pipeline que pipeline.py
        features = extract_all_features_original_pipeline(image_array)

        if features is not None:
            st.info(f"‚úÖ Extra√≠das {len(features)} caracter√≠sticas usando pipeline original")

            # Paso 2: Aplicar el MISMO preprocesamiento que durante el entrenamiento
            st.info("üîÑ Aplicando preprocesamiento (normalizaci√≥n + reducci√≥n dimensional)...")

            # Cargar pipeline de preprocesamiento
            preprocessing_pipeline = load_preprocessing_pipeline()

            # Aplicar preprocesamiento para obtener exactamente 10 caracter√≠sticas
            reduced_features = apply_preprocessing_pipeline(features, preprocessing_pipeline)

            if reduced_features is not None and len(reduced_features) == 10:
                st.success(f"‚úÖ Pipeline exitoso - {len(reduced_features)} caracter√≠sticas finales")

                # Debug: verificar dimensiones y compatibilidad
                with st.expander("üîß Debug - An√°lisis del Pipeline", expanded=False):
                    st.write(f"**Pipeline Original:**")
                    st.write(f"  - HSV: ~96 caracter√≠sticas")
                    st.write(f"  - LBP: ~26 caracter√≠sticas")
                    st.write(f"  - Hu Moments: 7 caracter√≠sticas")
                    st.write(f"  - Total extra√≠do: {len(features)} caracter√≠sticas")
                    st.write(f"**Preprocesamiento:**")
                    st.write(f"  - Normalizaci√≥n: StandardScaler")
                    st.write(f"  - Reducci√≥n: PCA + UMAP")
                    st.write(f"  - Caracter√≠sticas finales: {len(reduced_features)}")
                    st.write(f"**Caracter√≠sticas finales para el modelo:**")
                    for i, val in enumerate(reduced_features):
                        st.write(f"  Feature {i}: {val:.6f}")

                return reduced_features
            else:
                st.error("‚ùå Error: No se generaron exactamente 10 caracter√≠sticas")
                return None
        else:
            st.error("‚ùå Error en la extracci√≥n de caracter√≠sticas")
            return None

    except Exception as e:
        st.error(f"Error al procesar la imagen: {str(e)}")
        return None

def process_image_from_url_corrected(poster_url):
    """
    Procesa una imagen desde URL usando el MISMO PIPELINE que el modelo entrenado
    """
    try:
        # Descargar imagen
        img_array = download_img(poster_url)
        if img_array is None:
            return None

        # Extraer caracter√≠sticas usando el pipeline original
        features = extract_all_features_original_pipeline(img_array)

        if features is not None:
            # Aplicar preprocesamiento usando el mismo pipeline que durante entrenamiento
            preprocessing_pipeline = load_preprocessing_pipeline()
            reduced_features = apply_preprocessing_pipeline(features, preprocessing_pipeline)

            if reduced_features is not None and len(reduced_features) == 10:
                return reduced_features
            else:
                st.error("‚ùå Error en el preprocesamiento de la imagen desde URL")
                return None
        else:
            st.error("‚ùå Error en la extracci√≥n de caracter√≠sticas desde URL")
            return None

    except Exception as e:
        st.error(f"Error al procesar imagen desde URL {poster_url}: {str(e)}")
        return None

# 3. STREAMLIT APP CONFIGURACI√ìN
st.set_page_config(page_title="Movie Recommender", page_icon="üé¨", layout="wide")

st.title("üé¨ Sistema de Recomendaci√≥n de Pel√≠culas por Poster")
st.markdown("**Descubre pel√≠culas visualmente similares** usando K-means con 10 caracter√≠sticas visuales.")

@st.cache_resource
def load_model(model_path):
    try:
        with open(model_path, 'rb') as f:
            return pickle.load(f)
    except Exception as e:
        st.error(f"Error al cargar el modelo: {str(e)}")
        return None

@st.cache_data
def load_movie_data(csv_path):
    return pd.read_csv(csv_path)

def display_poster(url, title, width=150):
    """Muestra un poster desde URL o ruta local"""
    try:
        if pd.isna(url) or url == "":
            return False

        if url.startswith('http'):
            response = requests.get(url, timeout=10)
            img = Image.open(BytesIO(response.content))
        else:
            img = Image.open(url)

        st.image(img, caption=title, width=width)
        return True
    except Exception as e:
        st.warning(f"No se pudo cargar la imagen: {str(e)}")
        return False

class MovieClusteringAPI:
    """API simplificada para K-means con 10 features"""
    def __init__(self, analyzer):
        self.analyzer = analyzer

        # Verificar que el modelo tenga las propiedades necesarias
        if not hasattr(analyzer, 'movie_metadata'):
            raise ValueError("‚ùå El analizador debe tener movie_metadata")
        if not hasattr(analyzer, 'kmeans_model') or analyzer.kmeans_model is None:
            raise ValueError("‚ùå El analizador debe tener un modelo K-means entrenado")
        if not hasattr(analyzer, 'features') or analyzer.features.shape[1] != 10:
            raise ValueError(f"‚ùå El modelo debe usar exactamente 10 features, encontrados: {analyzer.features.shape[1] if hasattr(analyzer, 'features') else 'N/A'}")

        self.df_movies = analyzer.movie_metadata

    def search_similar_movies(self, movie_index, n_results=6):
        """Busca pel√≠culas similares bas√°ndose en distancia euclidiana global"""
        try:
            # Usar distancia euclidiana global en lugar de solo clusters
            ref_features = self.analyzer.features_scaled[movie_index]
            distances = []

            for idx in range(len(self.analyzer.features_scaled)):
                if idx != movie_index:  # Excluir la pel√≠cula de referencia
                    dist = np.linalg.norm(self.analyzer.features_scaled[idx] - ref_features)
                    distances.append((idx, dist))

            # Ordenar por distancia y tomar los m√°s cercanos
            distances.sort(key=lambda x: x[1])
            return [idx for idx, _ in distances[:n_results]]

        except Exception as e:
            st.error(f"Error al buscar pel√≠culas similares: {str(e)}")
            return []

    def search_similar_movies_by_features(self, image_features, n_results=6):
        """Busca pel√≠culas similares bas√°ndose en caracter√≠sticas de imagen (10 features)"""
        try:
            # Verificar que tenemos exactamente 10 features
            if len(image_features) != 10:
                raise ValueError(f"Se esperan exactamente 10 features, se recibieron {len(image_features)}")

            # Normalizar las caracter√≠sticas usando el mismo scaler del modelo
            image_features_scaled = self.analyzer.scaler.transform(image_features.reshape(1, -1))[0]

            # Calcular distancias con todas las pel√≠culas
            distances = []
            for idx in range(len(self.analyzer.features_scaled)):
                dist = np.linalg.norm(self.analyzer.features_scaled[idx] - image_features_scaled)
                distances.append((idx, dist))

            # Ordenar por distancia y devolver los m√°s similares
            distances.sort(key=lambda x: x[1])
            return [idx for idx, _ in distances[:n_results]]

        except Exception as e:
            st.error(f"Error al buscar pel√≠culas similares por caracter√≠sticas: {str(e)}")
            return []

    def search_similar_movies_by_title_and_poster(self, movie_title, poster_url, n_results=6):
        """Busca pel√≠culas similares descargando y procesando el poster desde URL"""
        try:
            st.info(f"üîÑ Procesando poster de '{movie_title}' desde URL...")

            # Procesar la imagen desde la URL
            image_features = process_image_from_url_corrected(poster_url)

            if image_features is not None and len(image_features) == 10:
                st.success("‚úÖ Caracter√≠sticas extra√≠das del poster (10 features)")
                return self.search_similar_movies_by_features(image_features, n_results=n_results)
            else:
                st.error("‚ùå No se pudo procesar el poster o no se generaron 10 caracter√≠sticas")
                return []

        except Exception as e:
            st.error(f"Error al buscar por t√≠tulo y poster: {str(e)}")
            return []

    def get_model_info(self):
        """Retorna informaci√≥n del modelo"""
        return {
            'total_movies': len(self.analyzer.features),
            'features_count': self.analyzer.features.shape[1],
            'n_clusters': self.analyzer.kmeans_model.n_clusters,
            'silhouette_score': getattr(self.analyzer, 'kmeans_silhouette', 'N/A')
        }

# 4. INTERFAZ DE USUARIO

# Configuraci√≥n de rutas por defecto corregidas - usando el modelo entrenado
default_model_path = "visual_clustering_model/visual_clustering_analyzer.pkl"
default_csv_path = "movie_data_complete.csv"

model_path = st.sidebar.text_input("Ruta al modelo (.pkl)", default_model_path)
csv_path = st.sidebar.text_input("Ruta a datos de pel√≠culas (.csv)", default_csv_path)

# Funci√≥n auxiliar para mostrar recomendaciones
def display_recommendations(similar_indices, title):
    """Funci√≥n auxiliar para mostrar recomendaciones"""
    if not similar_indices:
        st.warning("No se encontraron pel√≠culas similares.")
    else:
        st.subheader(f"{title} ({len(similar_indices)} encontradas)")
        cols = st.columns(3)

        for i, idx in enumerate(similar_indices):
            movie = df_movies.iloc[idx]
            with cols[i % 3]:
                movie_title = movie.get('title', f'Pel√≠cula {idx}')
                st.subheader(movie_title)

                # Mostrar poster usando poster_url preferentemente
                poster_displayed = False
                if 'poster_url' in movie and pd.notna(movie['poster_url']) and movie['poster_url'] != '':
                    poster_displayed = display_poster(movie['poster_url'], movie_title)
                elif 'poster_path' in movie and pd.notna(movie['poster_path']):
                    poster_displayed = display_poster(movie['poster_path'], movie_title)
                elif 'poster' in movie and pd.notna(movie['poster']):
                    poster_displayed = display_poster(movie['poster'], movie_title)

                if not poster_displayed:
                    st.warning("Poster no disponible")

                if 'movieId' in movie:
                    st.caption(f"ID: {movie['movieId']}")
                if 'genres' in movie:
                    st.caption(f"G√©neros: {movie['genres']}")
                if 'year' in movie:
                    st.caption(f"A√±o: {movie['year']}")

                # Mostrar enlace al poster
                if 'poster_url' in movie and pd.notna(movie['poster_url']) and movie['poster_url'] != '':
                    with st.expander("üîó Ver poster original"):
                        st.markdown(f"[Abrir poster]({movie['poster_url']})")

# Cargar modelo y datos
analyzer = load_model(model_path)
if analyzer is None:
    st.warning(f"No se pudo cargar el modelo desde: {model_path}")
    st.info("Aseg√∫rate de que la ruta sea correcta y el modelo est√© entrenado con el script corregido.")
    st.stop()

# Verificar que el modelo sea compatible
try:
    model_info = {
        'features_shape': analyzer.features.shape if hasattr(analyzer, 'features') else 'N/A',
        'has_kmeans': hasattr(analyzer, 'kmeans_model') and analyzer.kmeans_model is not None,
        'has_labels': hasattr(analyzer, 'kmeans_labels') and analyzer.kmeans_labels is not None
    }

    st.sidebar.markdown("### üîç Info del Modelo")
    st.sidebar.json(model_info)

    # Verificar caracter√≠sticas
    if hasattr(analyzer, 'features') and analyzer.features.shape[1] != 10:
        st.error(f"‚ùå MODELO INCOMPATIBLE: El modelo usa {analyzer.features.shape[1]} features, se esperan 10")
        st.info("Por favor, entrena el modelo con el script corregido que usa exactamente 10 features.")
        st.stop()

except Exception as e:
    st.error(f"Error al verificar el modelo: {str(e)}")
    st.stop()

try:
    df_movies = load_movie_data(csv_path)

    # Verificar estructura del CSV
    st.sidebar.markdown("### üìä Info del Dataset")
    st.sidebar.markdown(f"**Pel√≠culas:** {len(df_movies)}")
    st.sidebar.markdown(f"**Columnas:** {list(df_movies.columns)}")

    # Verificar columnas m√≠nimas necesarias
    if 'title' not in df_movies.columns:
        df_movies['title'] = [f'Pel√≠cula {i}' for i in range(len(df_movies))]
    if 'movieId' not in df_movies.columns:
        df_movies['movieId'] = df_movies.index

except Exception as e:
    st.error(f"Error al cargar datos de pel√≠culas: {str(e)}")
    st.stop()

# Crear API
try:
    api = MovieClusteringAPI(analyzer)
    model_info = api.get_model_info()

    st.sidebar.success("‚úÖ Modelo cargado correctamente")
    st.sidebar.markdown(f"**Pel√≠culas:** {model_info['total_movies']:,}")
    st.sidebar.markdown(f"**Features:** {model_info['features_count']} ‚úÖ")
    st.sidebar.markdown(f"**Clusters:** {model_info['n_clusters']}")
    st.sidebar.markdown(f"**Silhouette:** {model_info['silhouette_score']}")

except Exception as e:
    st.error(f"Error al crear API: {str(e)}")
    st.stop()

# 5. INTERFAZ PRINCIPAL

st.header("üîç Busca tu pel√≠cula")

# Crear tabs para diferentes m√©todos de b√∫squeda
tab1, tab2 = st.tabs(["üìù Buscar por t√≠tulo", "üñºÔ∏è Buscar por imagen"])

with tab1:
    search_term = st.text_input("Escribe el t√≠tulo de una pel√≠cula:", "")

    if search_term:
        matches = df_movies[df_movies['title'].str.contains(search_term, case=False, na=False)]

        if matches.empty:
            st.warning("No se encontraron pel√≠culas. Intenta con otro nombre.")
        else:
            selected_index = st.selectbox("Selecciona una pel√≠cula:",
                                         matches.index,
                                         format_func=lambda x: df_movies.loc[x, 'title'])

            selected_movie = df_movies.loc[selected_index]
            st.subheader(f"Pel√≠cula seleccionada: {selected_movie['title']}")

            # Mostrar informaci√≥n
            col1, col2 = st.columns([1, 3])
            with col1:
                poster_displayed = False
                if 'poster_url' in selected_movie and pd.notna(selected_movie['poster_url']) and selected_movie['poster_url'] != '':
                    poster_displayed = display_poster(selected_movie['poster_url'], selected_movie['title'], 200)
                elif 'poster_path' in selected_movie and pd.notna(selected_movie['poster_path']):
                    poster_displayed = display_poster(selected_movie['poster_path'], selected_movie['title'], 200)

                if not poster_displayed:
                    st.warning("Poster no disponible")

            with col2:
                st.markdown(f"**Movie ID:** {selected_movie.get('movieId', 'N/A')}")
                if 'genres' in selected_movie:
                    st.markdown(f"**G√©neros:** {selected_movie['genres']}")
                if 'year' in selected_movie:
                    st.markdown(f"**A√±o:** {selected_movie['year']}")

            # Botones de recomendaci√≥n
            col_a, col_b = st.columns(2)

            with col_a:
                if st.button("üéØ Similares por K-means", use_container_width=True, key="title_search_cluster"):
                    with st.spinner('Buscando recomendaciones por K-means...'):
                        similar_indices = api.search_similar_movies(selected_index)
                        display_recommendations(similar_indices, "üé¨ Pel√≠culas similares (K-means):")

            with col_b:
                if ('poster_url' in selected_movie and pd.notna(selected_movie['poster_url']) and
                    selected_movie['poster_url'] != ''):
                    if st.button("üñºÔ∏è Similares por Poster", use_container_width=True, key="title_search_poster"):
                        with st.spinner('Analizando poster (10 features)...'):
                            similar_indices = api.search_similar_movies_by_title_and_poster(
                                selected_movie['title'],
                                selected_movie['poster_url']
                            )
                            display_recommendations(similar_indices, "üé¨ Pel√≠culas similares (An√°lisis Visual):")
                else:
                    st.info("üñºÔ∏è An√°lisis visual no disponible (sin URL de poster)")

with tab2:
    st.markdown("**Sube la imagen de un poster** y encuentra pel√≠culas similares usando **10 caracter√≠sticas visuales**")

    uploaded_file = st.file_uploader(
        "Selecciona una imagen de poster:",
        type=['png', 'jpg', 'jpeg'],
        help="Sube una imagen de un poster de pel√≠cula para encontrar pel√≠culas similares"
    )

    # Informaci√≥n del pipeline actualizada
    st.markdown("### üî¨ Pipeline Optimizado para 10 Features")
    st.info("""
    **Sistema optimizado de extracci√≥n:**
    - **Histogramas HSV reducidos**: 24 caracter√≠sticas de color
    - **Local Binary Patterns**: 15 caracter√≠sticas de textura
    - **Estad√≠sticas b√°sicas**: 6 caracter√≠sticas complementarias
    - **Reducci√≥n PCA + UMAP**: Optimizaci√≥n a exactamente 10 features finales
    - **K-means clustering**: B√∫squeda por similitud euclidiana
    """)

    if uploaded_file is not None:
        # Mostrar la imagen subida
        image = Image.open(uploaded_file)
        col1, col2 = st.columns([1, 2])

        with col1:
            st.image(image, caption="Imagen subida", width=200)

        with col2:
            st.markdown("**Imagen cargada correctamente!**")
            st.markdown(f"- **Tama√±o:** {image.size}")
            st.markdown(f"- **Formato:** {image.format}")
            st.markdown(f"- **Modo:** {image.mode}")

        # Procesar imagen y buscar similares
        if st.button("üî¨ Buscar Pel√≠culas Similares", use_container_width=True, key="image_search"):
            with st.spinner('Procesando imagen con pipeline de 10 features...'):
                # Extraer caracter√≠sticas de la imagen usando el pipeline corregido
                image_features = process_uploaded_image_corrected(image)

                if image_features is not None and len(image_features) == 10:
                    # Buscar pel√≠culas similares usando las 10 caracter√≠sticas
                    similar_indices = api.search_similar_movies_by_features(image_features)

                    if not similar_indices:
                        st.warning("No se encontraron pel√≠culas similares.")
                    else:
                        st.subheader(f"üé¨ {len(similar_indices)} pel√≠culas m√°s similares:")
                        cols = st.columns(3)

                        for i, idx in enumerate(similar_indices):
                            movie = df_movies.iloc[idx]
                            with cols[i % 3]:
                                movie_title = movie.get('title', f'Pel√≠cula {idx}')
                                st.subheader(movie_title)

                                # Mostrar poster
                                poster_displayed = False
                                if 'poster_url' in movie and pd.notna(movie['poster_url']) and movie['poster_url'] != '':
                                    poster_displayed = display_poster(movie['poster_url'], movie_title)
                                elif 'poster_path' in movie and pd.notna(movie['poster_path']):
                                    poster_displayed = display_poster(movie['poster_path'], movie_title)

                                if not poster_displayed:
                                    st.warning("Poster no disponible")

                                if 'movieId' in movie:
                                    st.caption(f"ID: {movie['movieId']}")
                                if 'genres' in movie:
                                    st.caption(f"G√©neros: {movie['genres']}")
                                if 'year' in movie:
                                    st.caption(f"A√±o: {movie['year']}")

                                # Mostrar enlace al poster original
                                if 'poster_url' in movie and pd.notna(movie['poster_url']) and movie['poster_url'] != '':
                                    with st.expander("üîó Ver poster original"):
                                        st.markdown(f"[Abrir poster]({movie['poster_url']})")
                else:
                    st.error("‚ùå Error al procesar la imagen o no se generaron exactamente 10 caracter√≠sticas.")

        # Secci√≥n de ejemplo de posters para probar
        st.markdown("---")
        st.markdown("### üé≤ ¬øNo tienes una imagen? Prueba con estos ejemplos:")

        if st.button("üé¨ Mostrar algunos posters de ejemplo", key="show_examples"):
            example_movies = df_movies.head(6)  # Mostrar 6 ejemplos
            cols = st.columns(3)

            for i, (idx, movie) in enumerate(example_movies.iterrows()):
                with cols[i % 3]:
                    movie_title = movie.get('title', f'Pel√≠cula {idx}')
                    st.write(f"**{movie_title}**")
                    if 'poster_url' in movie and pd.notna(movie['poster_url']) and movie['poster_url'] != '':
                        display_poster(movie['poster_url'], movie_title, width=120)
                        st.caption("Puedes descargar y usar esta imagen")

# 6. INFORMACI√ìN ADICIONAL EN SIDEBAR

st.sidebar.header("üìù Instrucciones")
st.sidebar.markdown("""
**üéØ Modelo Corregido (10 Features + K-means):**
- ‚úÖ Usa exactamente 10 caracter√≠sticas visuales
- ‚úÖ Solo algoritmo K-means (simplificado)
- ‚úÖ B√∫squeda por distancia euclidiana
- ‚úÖ Compatible con prim_reduced.csv

**Buscar por t√≠tulo:**
1. Ve a "üìù Buscar por t√≠tulo"
2. Escribe el nombre de una pel√≠cula
3. Selecciona de la lista
4. Elige el m√©todo:
   - üéØ **K-means**: Usa el modelo entrenado
   - üñºÔ∏è **Por Poster**: Analiza visualmente

**Buscar por imagen:**
1. Ve a "üñºÔ∏è Buscar por imagen"
2. Sube una imagen de poster
3. Se extraen 10 caracter√≠sticas autom√°ticamente
4. Se buscan pel√≠culas similares
""")

st.sidebar.markdown("---")
st.sidebar.markdown("### üîß Caracter√≠sticas T√©cnicas")
st.sidebar.info("""
**Pipeline optimizado:**
- HSV reducido: 24 ‚Üí features de color
- LBP optimizado: 15 ‚Üí features de textura
- Stats b√°sicas: 6 ‚Üí features complementarias
- PCA + UMAP: 45 ‚Üí 10 features finales
- K-means: clustering con 10 features
""")

st.sidebar.markdown("---")
st.sidebar.markdown("### ‚öôÔ∏è Troubleshooting")

# Bot√≥n para verificar compatibilidad del modelo
if st.sidebar.button("üîç Verificar Modelo"):
    st.sidebar.markdown("**Verificando compatibilidad...**")

    checks = []

    # Check 1: N√∫mero de features
    if hasattr(analyzer, 'features'):
        features_shape = analyzer.features.shape
        checks.append(f"‚úÖ Features: {features_shape[1]}/10" if features_shape[1] == 10 else f"‚ùå Features: {features_shape[1]}/10")
    else:
        checks.append("‚ùå No se encontraron features")

    # Check 2: Modelo K-means
    if hasattr(analyzer, 'kmeans_model') and analyzer.kmeans_model is not None:
        n_clusters = analyzer.kmeans_model.n_clusters
        checks.append(f"‚úÖ K-means: {n_clusters} clusters")
    else:
        checks.append("‚ùå Modelo K-means no encontrado")

    # Check 3: Labels
    if hasattr(analyzer, 'kmeans_labels') and analyzer.kmeans_labels is not None:
        checks.append(f"‚úÖ Labels: {len(analyzer.kmeans_labels)} pel√≠culas")
    else:
        checks.append("‚ùå Labels no encontradas")

    # Check 4: Scaler
    if hasattr(analyzer, 'scaler'):
        checks.append("‚úÖ Scaler disponible")
    else:
        checks.append("‚ùå Scaler no encontrado")

    for check in checks:
        st.sidebar.markdown(check)

# Bot√≥n para mostrar estad√≠sticas del dataset
if st.sidebar.button("üìä Stats del Dataset"):
    st.sidebar.markdown("**Estad√≠sticas del dataset:**")
    st.sidebar.markdown(f"- Total pel√≠culas: {len(df_movies)}")

    if 'poster_url' in df_movies.columns:
        valid_urls = df_movies['poster_url'].notna().sum()
        st.sidebar.markdown(f"- URLs v√°lidas: {valid_urls}/{len(df_movies)}")

    if 'title' in df_movies.columns:
        valid_titles = df_movies['title'].notna().sum()
        st.sidebar.markdown(f"- T√≠tulos v√°lidos: {valid_titles}/{len(df_movies)}")

    st.sidebar.markdown(f"- Columnas: {len(df_movies.columns)}")

st.sidebar.markdown("---")
st.sidebar.caption("üé¨ Movie Recommender | Versi√≥n Corregida | 10 Features + K-means")

# 7. FOOTER CON INFORMACI√ìN ADICIONAL

# Definir informaci√≥n b√°sica del modelo
model_info = {
    'total_movies': len(df_movies) if 'df_movies' in locals() else 0,
    'features_count': 10,
    'n_clusters': 8
}

st.markdown("---")
st.markdown("### üìã Informaci√≥n del Sistema")

col1, col2, col3 = st.columns(3)

with col1:
    st.metric("Pel√≠culas Analizadas", f"{model_info['total_movies']:,}")

with col2:
    st.metric("Caracter√≠sticas Visuales", f"{model_info['features_count']}")

with col3:
    st.metric("Clusters K-means", f"{model_info['n_clusters']}")

# Mostrar informaci√≥n adicional sobre el modelo en un expander
with st.expander("üî¨ Detalles T√©cnicos del Modelo", expanded=False):
    st.markdown("""
    **Arquitectura del Sistema:**

    1. **Extracci√≥n de Caracter√≠sticas (45 ‚Üí 10)**:
       - Histogramas HSV con bins reducidos (24 features)
       - Local Binary Patterns optimizados (15 features)
       - Estad√≠sticas b√°sicas de color (6 features)
       - Reducci√≥n dimensional PCA + UMAP a 10 features finales

    2. **Algoritmo de Clustering**:
       - K-means personalizado implementado desde cero
       - Inicializaci√≥n K-means++ para mejor convergencia
       - Optimizaci√≥n por silhouette score

    3. **B√∫squeda de Similitud**:
       - Distancia euclidiana en espacio normalizado
       - B√∫squeda global (no limitada a clusters)
       - Escalado autom√°tico con StandardScaler

    4. **Pipeline de Procesamiento**:
       - Conversi√≥n autom√°tica RGB ‚Üí HSV
       - Extracci√≥n paralela de caracter√≠sticas
       - Normalizaci√≥n y reducci√≥n dimensional
       - Predicci√≥n en tiempo real
    """)

    if hasattr(analyzer, 'kmeans_silhouette'):
        st.markdown(f"**M√©tricas de Calidad:**")
        st.markdown(f"- Silhouette Score: {analyzer.kmeans_silhouette:.4f}")
        if hasattr(analyzer, 'kmeans_inertia'):
            st.markdown(f"- Inercia: {analyzer.kmeans_inertia:.2f}")

# Mensaje final
st.success("üéâ Sistema listo para usar! Prueba subiendo una imagen o buscando por t√≠tulo.")

# Nota importante sobre compatibilidad
st.info("""
üí° **Nota importante**: Este sistema est√° optimizado para trabajar con el modelo corregido que usa
exactamente 10 caracter√≠sticas visuales. Si tienes errores, aseg√∫rate de:

1. Usar el modelo entrenado con el script corregido
2. Que el archivo CSV tenga las columnas correctas
3. Que las im√°genes sean de posters de pel√≠culas para mejores resultados
""")

# Easter egg: bot√≥n para mostrar algunas recomendaciones aleatorias
if st.button("üé≤ Descubre Pel√≠culas Aleatorias", key="random_discovery"):
    st.markdown("### üé¨ Descubrimiento Aleatorio")
    random_indices = np.random.choice(len(df_movies), size=6, replace=False)

    cols = st.columns(3)
    for i, idx in enumerate(random_indices):
        movie = df_movies.iloc[idx]
        with cols[i % 3]:
            movie_title = movie.get('title', f'Pel√≠cula {idx}')
            st.subheader(movie_title)

            if 'poster_url' in movie and pd.notna(movie['poster_url']) and movie['poster_url'] != '':
                display_poster(movie['poster_url'], movie_title, width=120)

            if st.button(f"Ver similares", key=f"random_{idx}"):
                with st.spinner(f'Buscando pel√≠culas similares a {movie_title}...'):
                    similar_indices = api.search_similar_movies(idx)
                    display_recommendations(similar_indices, f"Similares a {movie_title}:")
